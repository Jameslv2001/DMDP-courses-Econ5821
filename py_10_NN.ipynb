{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "### Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural network (ANN) is the workhorse of AI\n",
    "* A type of nonlinear models (with a structure)\n",
    "\n",
    "![NN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = g^{(k)} ( z_l^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choice: sigmoid ($1/(1+\\exp(-x))$); ReLu, $z\\cdot 1\\{x\\geq 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "A user has several decisions to make\n",
    "* Activation function\n",
    "* Number of hidden layers\n",
    "* Number of nodes in each layer\n",
    "\n",
    "\n",
    "* Many free parameters are generated from the multiple layer and multiple nodes\n",
    "* Regularization methods can be employed to penalize\n",
    "the $l_1$ and/or $l_2$ norms, which requires extra tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does It Work?\n",
    "\n",
    "* Animated video by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Theory is Incomplete\n",
    "\n",
    "* Theoretical understanding is an ongoing endeavor.\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "    * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "* Free parameters must be determined by numerical optimization\n",
    "* Nonlinear multiplicative structure makes the optimization challenging\n",
    "* Secure the global optimizer is almost impossible\n",
    "* De facto optimization algorithm is *stochastic gradient descent*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Google's `tensorflow`\n",
    "* `keras` is the deep learning modeling language"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
