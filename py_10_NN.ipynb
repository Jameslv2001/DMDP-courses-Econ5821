{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "### Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "* Neural network is the workhorse of AI\n",
    "* A type of nonlinear models (with a structure)\n",
    "\n",
    "![NN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = b_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = g^{(k)} ( z_l^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choices\n",
    "  * Sigmoid: ($1/(1+\\exp(-x))$)\n",
    "  * Rectified linear unit (ReLu) $z\\cdot 1\\{x\\geq 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Does It Work?\n",
    "\n",
    "* Animated video by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "* Feedforward: criterion evaluation\n",
    "* Back propagation: parameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* One-layer feedforward NN for demonstration\n",
    "* Input: $p$\n",
    "* Hidden nodes: $K$\n",
    "  \n",
    "* Criterion: \n",
    "$$\n",
    "\\min_{\\theta}   \\frac{1}{2}\\sum_{i=1}^n  Q_i \\textrm{ where } Q_i = [y_i - f^{(2)}(X_i) ]^2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "f^{(2)}(X_i) & =  \\beta^{(2)} + \\sum_{j=1}^K w_{j}^{(2)} \\sigma \\left( z_j\\right) \\\\\n",
    "z_j & =\\beta_j^{(1)} + \\sum_{\\ell=1}^p w_{j\\ell}^{(1)} x_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient method\n",
    "\n",
    "Taylor expansion\n",
    "\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) (\\theta_{t+1}-\\theta_{t}) + h.o.t.\n",
    "$$\n",
    "where\n",
    "* $\\nabla Q(\\theta_t)$ is **Gradient**\n",
    "* $(\\theta_{t+1}-\\theta_{t})$ is unknown, use $p_t$ (**length of step**) to replace it as\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) p_t\n",
    "$$\n",
    "* Choose $p_k = - \\alpha \\cdot \\nabla Q(\\theta_t)$ ensures reduction in $Q$, where $\\alpha$ is the **learning rate**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Output layer -> hidden layer\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\sigma\\left(z_{j}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* Hidden layer -> input layer: by the chain rule \n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)x_{i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "* Large n\n",
    "* Sample a *minibatch*\n",
    "  * Unbiased gradient, but large variance\n",
    "* Learning rate\n",
    "* Many epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* $L_1$-norm (Lasso)\n",
    "* $L_2$-norm (ridge)\n",
    "* Learning rate\n",
    "* Number of epochs and minibatches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks\n",
    "\n",
    "* Google's `Tensorflow`\n",
    "  * Keras: high level, easy to implement\n",
    "* Meta's `pytorch`\n",
    "  * Literal style\n",
    "  * Easy to use/reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation Example\n",
    "\n",
    "* Use NN to solve Poisson regression\n",
    "  * A trivial example for demonstration\n",
    "  * No hidden layer\n",
    "  * Keep the essence\n",
    "  \n",
    "* See `data_example/nn_Poisson_Keras_HD.ipynb`\n",
    "* See `data_example/nn_torch.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Structures\n",
    "\n",
    "* Time series\n",
    "  * Recurrent NN (RNN)\n",
    "  * Long term and short term memory (LSTM) (See `data_example/nn_LSTM.ipynb`)\n",
    "* Graphics\n",
    "  * Convolutional NN (CNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Theory is Incomplete\n",
    "\n",
    "* Theoretical understanding is an ongoing endeavor.\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "    * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Conflict\n",
    "\n",
    "* Collaborators working separately on the same line of a file may easily encounter conflicts. \n",
    "* If so, `git pull` will not merge changes from the remote into local repo. \n",
    "* Need to resolve the conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markdown\n",
    "\n",
    "* Text only. \n",
    "* Simple syntax.\n",
    "* [Cheat sheet](https://www.markdownguide.org/cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Initial a git repo in a folder (either local or on SCRP)\n",
    "2. Create an md file called `hello.md`, and write \"Hello World!\" in it.\n",
    "3. Commit the changes.\n",
    "4. Register a github account (if you haven't done so).\n",
    "5. Push the latest commit to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Readings\n",
    "\n",
    "As a fundamental tool for coding, there are plenty of excellent tutorials online.\n",
    "\n",
    "<!-- * Grant McDermott's lecture notes: [Git](https://raw.githack.com/uo-ec510-2020-spring/lectures/master/02-git/02-git.html#1), [Shell](https://raw.githack.com/uo-ec510-2020-spring/lectures/master/03-shell/03-shell.html#1) -->\n",
    "* Youtube videos\n",
    "  * 10-min [video](https://www.youtube.com/watch?v=q4CQBuZ4IGo) in Chinese.\n",
    "  * 1-hour [video](https://www.youtube.com/watch?v=RGOj5yH7evk) for beginners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Extra reading**\n",
    "\n",
    "* [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Final words**: \n",
    "Don't expect you can be a Git guru in one day.\n",
    "It is learning by doing."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "rise": {
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
