\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{lstautogobble}

\lstset{language=Python,
        basicstyle=\ttfamily\small,
        keywordstyle=\bfseries\color{blue},
        stringstyle=\color{red},
        commentstyle=\color{green},
        morekeywords={@, if, for, else, elif},
        emph={np, pd, matplotlib, plt, seaborn, sns},
        emphstyle=\color{purple},
        showstringspaces=false,
        breaklines=true,
        captionpos=t,
        frame=lines,
        tabsize=4,
        numbers=left,
        numberstyle=\tiny,
        stepnumber=5,
        numbersep=5pt
}

\title{Data Science for Economist Py 04 Advanced}
\author{James Lyu}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document provides a detailed explanation of implementing a linear probability model and calculating the robust variance-covariance matrix using different methods. The code is explained step-by-step, and the differences between dense and sparse matrix multiplication are discussed.

\section{Understanding the `lpm` Function}

The following Python function implements a linear probability model and returns the design matrix \(X\) and the residuals \(e_{\text{hat}}\):

\begin{lstlisting}[caption={Linear Probability Model Function}, label={lst:lpm_function}]
def lpm(n):
    # estimation in a linear probability model
    # set the parameters
    b0 = np.array([-1, 1])
    # generate the data
    e = np.random.normal(size=n)
    X = np.hstack((np.ones((n, 1)), np.random.normal(size=(n, 1))))
    Y = (X @ b0 + e >= 0)
    # OLS estimation
    bhat = np.linalg.inv(X.T @ X) @ X.T @ Y
    # calculate the t-value
    bhat2 = bhat[1]  # parameter we want to test
    e_hat = Y - X @ bhat
    return X, e_hat
\end{lstlisting}

\subsection{Explanation of the `lpm` Function}
\begin{itemize}
    \item \texttt{b0}: The true parameter vector \([-1, 1]\).
    \item \texttt{e}: Random errors following a normal distribution.
    \item \texttt{X}: Design matrix with a column of ones (intercept) and random predictors.
    \item \texttt{Y}: Binary response variable generated by thresholding the linear prediction at zero.
    \item \texttt{bhat}: OLS estimate of the parameters.
    \item \texttt{e\_hat}: Residuals from the model.
    \item The function returns the design matrix \(X\) and the residuals \(e_{\text{hat}}\).
\end{itemize}

\section{Robust Variance Matrix Calculation}

The following code compares three methods for calculating the robust variance-covariance matrix:

\begin{lstlisting}[caption={Robust Variance Matrix Calculation}, label={lst:variance_matrix}]
import time
import numpy as np
from scipy.sparse import diags

# Example parameters
n = 10  # Number of observations
Rep = 5000  # Number of replications

for opt in range(1, 4):
    pts0 = time.time()
    XXe2 = np.zeros((2, 2))
    
    for iter in range(Rep):
        np.random.seed(iter)
        X, e_hat = lpm(n)
        
        if opt == 1:
            # Element-by-element calculation
            for i in range(n):
                XXe2 += e_hat[i]**2 * np.matmul(X[i,].T, X[i,])
        elif opt == 2:
            # Dense matrix multiplication
            e_hat2_M = np.zeros((n, n))
            np.fill_diagonal(e_hat2_M, e_hat**2)
            XXe2 = X.T @ e_hat2_M @ X
        elif opt == 3:
            # Sparse matrix multiplication
            e_hat2_M = diags(e_hat**2, format='csr')
            XXe2 = X.T @ e_hat2_M @ X
    
    print(f"n = {n}, Rep = {Rep}, opt = {opt}, time = {time.time() - pts0:.4f} seconds")
\end{lstlisting}

\subsection{Explanation of the Robust Variance Matrix Calculation}
\begin{itemize}
    \item \texttt{n}: Number of observations in the dataset.
    \item \texttt{Rep}: Number of replications for the calculation.
    \item \texttt{opt}: Option for choosing the calculation method:
        \begin{itemize}
            \item \texttt{opt=1}: Element-by-element calculation.
            \item \texttt{opt=2}: Dense matrix multiplication.
            \item \texttt{opt=3}: Sparse matrix multiplication.
        \end{itemize}
    \item \texttt{XXe2}: Initialized as a zero matrix to accumulate the variance-covariance matrix.
    \item \texttt{np.random.seed(iter)}: Ensures reproducibility by setting a random seed.
    \item \texttt{X, e\_hat = lpm(n)}: Generates the design matrix and residuals using the `lpm` function.
    \item Depending on the chosen option, the variance-covariance matrix is calculated using:
        \begin{itemize}
            \item Element-wise multiplication and accumulation.
            \item Dense matrix multiplication.
            \item Sparse matrix multiplication for efficiency.
        \end{itemize}
    \item The execution time for each method is printed for comparison.
\end{itemize}

\section{Differences Between Dense and Sparse Matrix Multiplication}

\subsection{Dense Matrix Multiplication}
\begin{itemize}
    \item Stores all elements of the matrix, including zeros.
    \item Example code:
    \begin{lstlisting}[caption={Dense Matrix Example}]
    e_hat2_M = np.zeros((n, n))
    np.fill_diagonal(e_hat2_M, e_hat**2)
    XXe2 = X.T @ e_hat2_M @ X
    \end{lstlisting}
    \item Advantages:
        \begin{itemize}
            \item Straightforward implementation.
            \item No need for additional libraries.
        \end{itemize}
    \item Disadvantages:
        \begin{itemize}
            \item High memory usage for large \(n\).
            \item Computationally expensive for large matrices.
        \end{itemize}
\end{itemize}

\subsection{Sparse Matrix Multiplication}
\begin{itemize}
    \item Stores only non-zero elements, reducing memory usage.
    \item Example code:
    \begin{lstlisting}[caption={Sparse Matrix Example}]
    e_hat2_M = diags(e_hat**2, format='csr')
    XXe2 = X.T @ e_hat2_M @ X
    \end{lstlisting}
    \item Advantages:
        \begin{itemize}
            \item Memory efficient for large matrices.
            \item Faster computation for sparse matrices.
        \end{itemize}
    \item Disadvantages:
        \begin{itemize}
            \item Requires additional libraries (e.g., `scipy.sparse`).
            \item Slightly more complex implementation.
        \end{itemize}
\end{itemize}

\subsection{Comparison Summary}
\begin{itemize}
    \item \textbf{Dense Matrix Multiplication}:
        \begin{itemize}
            \item Suitable for small to moderate-sized matrices.
            \item Easy to implement but less efficient for large datasets.
        \end{itemize}
    \item \textbf{Sparse Matrix Multiplication}:
        \begin{itemize}
            \item Ideal for large datasets with sparse structures.
            \item More efficient in terms of memory and computation but requires specialized libraries.
        \end{itemize}
\end{itemize}

\section{Conclusion}
This document provided a detailed explanation of implementing a linear probability model and calculating the robust variance-covariance matrix using different methods. The key differences between dense and sparse matrix multiplication were discussed, highlighting their respective advantages and disadvantages. The choice of method depends on the size and sparsity of the dataset.

\end{document}